{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657bbd30",
   "metadata": {},
   "source": [
    "### Предсказание дефолта сделки, используя Logistic Regression, SVM, Random Forest Classifier, Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc27907",
   "metadata": {},
   "source": [
    "Файл с тренировочными данными с таргетной переменной лежит в train.csv. Файл с тестовыми данными без таргетной переменной лежит в test.csv\n",
    "\n",
    "Задача  - построить алгоритм, определяющий вероятность дефолта того или иного заказа на исторических данных. Бейзлайн точности на тестовых данных 70%. Проверка проходила на стороне чекера, итоговая модель выдала точность 71%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6ed13",
   "metadata": {},
   "source": [
    "## Оглавление\n",
    "1. [Импорт библиотек](#Импорт-библиотек)\n",
    "1. [Импорт датасетов](#Импорт-датасетов)\n",
    "2. [Смотрим и заполняем пропуски](#Смотрим-и-заполняем-пропуски)\n",
    "3. [TargetEncoder колонки \"Region\"](#TargetEncoder-колонки-\"Region\")\n",
    "1. [One Hot Encoding](#One-Hot-Encoding\")\n",
    "1. [Генерируем новые фичи](#Генерируем-новые-фичи)\n",
    "1. [Удаляем незначимые для таргета колонки](#Удаляем-незначимые-для-таргета-колонки)\n",
    "3. [Разделим ранее сконкатенированные датасеты](#Разделим-ранее-сконкатенированные-датасеты)\n",
    "1. [Разделим датасет на трейн и тест с параметром stratify](#Разделим-датасет-на-трейн-и-тест-с-параметром-stratify\")\n",
    "1. [Построим базовую модель LogisticRegression в качестве бейзлайна.](#Построим-базовую-модель-LogisticRegression-в-качестве-бейзлайна)\n",
    "1. [Построим пайпланы для SVM, RandomForest, Catboost](#Построим-пайпланы-для-SVM,-RandomForest,-Catboost)\n",
    "1. [SVM](#SVM)\n",
    "1. [RandomForest](#RandomForest)\n",
    "1. [Смотрим на значимость признаков для модели](#Смотрим-на-значимость-признаков-для-модели)\n",
    "1. [Catboost](#Catboost)\n",
    "1. [Сохранение датасета](#Сохранение-датасета)\n",
    "1. [В какую сторону можно искать улучшения?](#В-какую-сторону-можно-искать-улучшения?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03342ec7",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e98d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24968648",
   "metadata": {},
   "source": [
    "### Импорт датасетов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719300fc",
   "metadata": {},
   "source": [
    "Соединим test и train датасеты чтобы лучше понять распределение данных в датасетах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df70cd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deal_id</th>\n",
       "      <th>Deal_date</th>\n",
       "      <th>First_deal_date</th>\n",
       "      <th>Secret_dwarf_info_1</th>\n",
       "      <th>Secret_dwarf_info_2</th>\n",
       "      <th>Secret_dwarf_info_3</th>\n",
       "      <th>First_default_date</th>\n",
       "      <th>Successful_deals_count</th>\n",
       "      <th>Region</th>\n",
       "      <th>Tavern</th>\n",
       "      <th>Hashed_deal_detail_1</th>\n",
       "      <th>Hashed_deal_detail_2</th>\n",
       "      <th>Hashed_deal_detail_3</th>\n",
       "      <th>Hashed_deal_detail_4</th>\n",
       "      <th>Hashed_deal_detail_5</th>\n",
       "      <th>Hashed_deal_detail_6</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22487461</td>\n",
       "      <td>2015-11-05</td>\n",
       "      <td>2015-08-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tavern_district_3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62494261</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>2015-12-21</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-07-30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tavern_district_4</td>\n",
       "      <td>7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>14</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34822849</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>2015-11-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tavern_district_6</td>\n",
       "      <td>7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46893387</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tavern_district_2</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67128275</td>\n",
       "      <td>2016-09-19</td>\n",
       "      <td>2016-07-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tavern_district_4</td>\n",
       "      <td>39</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deal_id   Deal_date First_deal_date  Secret_dwarf_info_1  \\\n",
       "0  22487461  2015-11-05      2015-08-29                  NaN   \n",
       "1  62494261  2016-08-26      2015-12-21                  3.5   \n",
       "2  34822849  2016-02-18      2015-11-11                  NaN   \n",
       "3  46893387  2016-04-30      2016-03-22                  NaN   \n",
       "4  67128275  2016-09-19      2016-07-21                  NaN   \n",
       "\n",
       "   Secret_dwarf_info_2  Secret_dwarf_info_3 First_default_date  \\\n",
       "0                  NaN                  NaN                NaN   \n",
       "1                 -2.0                  5.0         2016-07-30   \n",
       "2                  NaN                  NaN                NaN   \n",
       "3                  NaN                  NaN                NaN   \n",
       "4                  NaN                  NaN                NaN   \n",
       "\n",
       "   Successful_deals_count             Region  Tavern  Hashed_deal_detail_1  \\\n",
       "0                     0.0  Tavern_district_3       7                   2.5   \n",
       "1                     2.0  Tavern_district_4       7                   2.5   \n",
       "2                     0.0  Tavern_district_6       7                   2.5   \n",
       "3                     0.0  Tavern_district_2      13                   2.5   \n",
       "4                     0.0  Tavern_district_4      39                   2.5   \n",
       "\n",
       "   Hashed_deal_detail_2  Hashed_deal_detail_3  Hashed_deal_detail_4  \\\n",
       "0                    -3                     8                   2.5   \n",
       "1                    -3                    14                   3.5   \n",
       "2                    -3                     8                   2.5   \n",
       "3                    -2                     5                   2.5   \n",
       "4                    -3                     7                   2.5   \n",
       "\n",
       "   Hashed_deal_detail_5  Hashed_deal_detail_6   Age  Gender  Default  \n",
       "0                    -3                     5  36.0    Male      0.0  \n",
       "1                    -3                     5  29.0  Female      1.0  \n",
       "2                    -3                     5  56.0  Female      0.0  \n",
       "3                    -3                     5  27.0  Female      0.0  \n",
       "4                    -3                     5  37.0  Female      0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./train.csv\")\n",
    "df = df.loc[df['Default'].notna()]\n",
    "df_test = pd.read_csv(\"./test.csv\")\n",
    "df = pd.concat([df, df_test], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33beb02",
   "metadata": {},
   "source": [
    "### Смотрим и заполняем пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05065976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deal_id                      0\n",
       "Deal_date                    0\n",
       "First_deal_date              0\n",
       "Secret_dwarf_info_1       3482\n",
       "Secret_dwarf_info_2       3482\n",
       "Secret_dwarf_info_3       3482\n",
       "First_default_date        3482\n",
       "Successful_deals_count      15\n",
       "Region                      11\n",
       "Tavern                       0\n",
       "Hashed_deal_detail_1         0\n",
       "Hashed_deal_detail_2         0\n",
       "Hashed_deal_detail_3         0\n",
       "Hashed_deal_detail_4         0\n",
       "Hashed_deal_detail_5         0\n",
       "Hashed_deal_detail_6         0\n",
       "Age                          0\n",
       "Gender                       0\n",
       "Default                    989\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6b7dd",
   "metadata": {},
   "source": [
    "Заполним пустые ячейки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe3cec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Tavern_district_3\n",
       "Name: Region, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получим моду колонки Region\n",
    "df.Region.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23065d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Secret_dwarf_info_2\n",
       "-2.0    314\n",
       "-3.0    281\n",
       "-1.0     66\n",
       " 0.0      9\n",
       " 1.0      3\n",
       " 2.0      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Secret_dwarf_info_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78377cd3",
   "metadata": {},
   "source": [
    "так как в столбце 'Secret_dwarf_info_2' имеется значение 0.0, заполним его отличным от всех значением \n",
    "для дальнейшего применения OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef4b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Secret_dwarf_info_1'].fillna(0, inplace=True)\n",
    "df['Secret_dwarf_info_2'].fillna(100, inplace=True)\n",
    "df['Secret_dwarf_info_3'].fillna(0, inplace=True)\n",
    "df['First_default_date'].fillna('1970-01-01', inplace=True)\n",
    "df['Successful_deals_count'].fillna(0, inplace=True)\n",
    "df['Region'].fillna('Tavern_district_3', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a6ac5",
   "metadata": {},
   "source": [
    "проверим, что заполнены все ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4123eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deal_id                     0\n",
       "Deal_date                   0\n",
       "First_deal_date             0\n",
       "Secret_dwarf_info_1         0\n",
       "Secret_dwarf_info_2         0\n",
       "Secret_dwarf_info_3         0\n",
       "First_default_date          0\n",
       "Successful_deals_count      0\n",
       "Region                      0\n",
       "Tavern                      0\n",
       "Hashed_deal_detail_1        0\n",
       "Hashed_deal_detail_2        0\n",
       "Hashed_deal_detail_3        0\n",
       "Hashed_deal_detail_4        0\n",
       "Hashed_deal_detail_5        0\n",
       "Hashed_deal_detail_6        0\n",
       "Age                         0\n",
       "Gender                      0\n",
       "Default                   989\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c7d6f",
   "metadata": {},
   "source": [
    "### TargetEncoder колонки \"Region\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff908e2",
   "metadata": {},
   "source": [
    "экспериментальным путем выяснили, что это лучший вариант обработки колонки это применение TargetEncoder к колонке \"Region\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6217127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Default']\n",
    "x=df['Region']\n",
    "\n",
    "df['Region'] = ce.TargetEncoder().fit_transform(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7149337",
   "metadata": {},
   "source": [
    "### Отмасштабируем колонку \"Age\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592aec0",
   "metadata": {},
   "source": [
    "Колонку Age отмасштабируем, такая техника часто может повысить качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b404f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age']//30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aff7a2",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa8ecc",
   "metadata": {},
   "source": [
    "Определим какие колонки мы хотим кодировать OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16de18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_cols = ['Age', 'Gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598b76a",
   "metadata": {},
   "source": [
    "Приведем даты к типу datetime, сгенерируем на основе них новые категориальные фичи year, month, day, weekday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca391a",
   "metadata": {},
   "source": [
    "Применим OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded41599",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['Deal_date', 'First_deal_date', 'First_default_date'] \n",
    "\n",
    "fddt = []    \n",
    "for i in range(len(df['First_default_date'])):\n",
    "    if not df['First_default_date'][i]:\n",
    "        fddt.append(df['Deal_date'][i])\n",
    "    else:\n",
    "        fddt.append(df['First_default_date'][i])\n",
    "df['First_default_date'] = fddt\n",
    "    \n",
    "for col in date_cols:\n",
    "    df[col] = df[col].apply(pd.to_datetime)\n",
    "    df[f'{col}_time'] = df[col].values.astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    if col == 'First_default_date':\n",
    "        continue\n",
    "\n",
    "    df[f'{col}_year'] = df[col].dt.year\n",
    "    dummies_cols.append(f'{col}_year')\n",
    "    \n",
    "    df[f'{col}_month'] = df[col].dt.month\n",
    "    dummies_cols.append(f'{col}_month')\n",
    "    \n",
    "    if col == 'First_deal_date':\n",
    "        continue\n",
    "    \n",
    "    df[f'{col}_weekday'] = df[col].dt.weekday\n",
    "    dummies_cols.append(f'{col}_weekday')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=dummies_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd197ab0",
   "metadata": {},
   "source": [
    "### Генерируем новые фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8f556",
   "metadata": {},
   "source": [
    "Сгенерируем новые фичи - разницы между датами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d463bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fdd-Dd'] = (df['Deal_date_time'] - df['First_deal_date_time']).astype('int64')\n",
    "df['Ff'] = (df['First_default_date_time'] - df['First_deal_date_time']).astype('int64')\n",
    "df['Dd-fdd'] = (df['Deal_date_time'] - df['First_default_date_time']).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b722f9",
   "metadata": {},
   "source": [
    "Сгенерируем новый синтетический признак, выявлен экспериментальным путем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28cdc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hh'] = df['Hashed_deal_detail_3']* df['Hashed_deal_detail_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16da8ce",
   "metadata": {},
   "source": [
    "### Удаляем незначимые для таргета колонки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fe99e",
   "metadata": {},
   "source": [
    "Колонка Hashed_deal_detail_6 имеет одно значение поэтому ее удалим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16fc7acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hashed_deal_detail_6\n",
       "5    4157\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Hashed_deal_detail_6'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc62932",
   "metadata": {},
   "source": [
    "Удалим малозначимые колонки, которыем мы выявили методом feature_importances_ уже после обучения модели в процессе донастройки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb4df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Deal_id', 'Deal_date', 'First_deal_date', 'First_default_date', 'Deal_date_time', 'Tavern', 'Deal_date_year_2017',\n",
    "         'Hashed_deal_detail_2', 'Hashed_deal_detail_1', 'Hashed_deal_detail_5', 'Hashed_deal_detail_6'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5204381",
   "metadata": {},
   "source": [
    "### Разделим ранее сконкатенированные датасеты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded1b8f",
   "metadata": {},
   "source": [
    "Отделим test датасет без таргета, который мы сконкатенировали в 1 шаге для изучения распределения данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98ca78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[df['Default'].isna()]\n",
    "\n",
    "df = df.loc[df['Default'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b398d",
   "metadata": {},
   "source": [
    "### Разделим датасет на трейн и тест с параметром stratify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27253ab",
   "metadata": {},
   "source": [
    "Разделим датасет на трейн и тест используя stratify для балансировки классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da8d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['Default']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed234ca6",
   "metadata": {},
   "source": [
    "Убедимся, что распределение таргета в датасетах одинаковое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c8e3c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Доля в трейне</th>\n",
       "      <th>Доля в тесте</th>\n",
       "      <th>Абсолютная разница</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.889108</td>\n",
       "      <td>0.88959</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.110892</td>\n",
       "      <td>0.11041</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Доля в трейне  Доля в тесте  Абсолютная разница\n",
       "Default                                                 \n",
       "0.0           0.889108       0.88959            0.000482\n",
       "1.0           0.110892       0.11041            0.000482"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shares = df_train[\"Default\"].value_counts() / df_train.shape[0]\n",
    "test_shares = df_test[\"Default\"].value_counts() / df_test.shape[0]\n",
    "\n",
    "to_compare = pd.concat((train_shares, test_shares), axis=1)\n",
    "to_compare.columns = ['Доля в трейне', 'Доля в тесте']\n",
    "to_compare['Абсолютная разница'] = (to_compare[\"Доля в трейне\"] - \\\n",
    "                                    to_compare[\"Доля в тесте\"]).abs()\n",
    "\n",
    "to_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c95e6",
   "metadata": {},
   "source": [
    "### Построим базовую модель LogisticRegression в качестве бейзлайна. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e94e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на тренировочной выборке составило 0.891\n",
      "Accuracy на тестовой выборке составило 0.886\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr = Pipeline(\n",
    "    [\n",
    "        ('stc', StandardScaler()),\n",
    "        ('lr', LogisticRegression(random_state=100))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_lr.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "train_preds = pipeline_lr.predict(df_train.drop('Default', axis=1)) \n",
    "train_accuracy = np.mean(train_preds == df_train['Default'].values)\n",
    "\n",
    "test_preds = pipeline_lr.predict(df_test.drop('Default', axis=1)) \n",
    "test_accuracy = np.mean(test_preds == df_test['Default'].values)\n",
    "\n",
    "print(f\"Accuracy на тренировочной выборке составило {np.round(train_accuracy, decimals=3)}\")\n",
    "print(f\"Accuracy на тестовой выборке составило {np.round(test_accuracy, decimals=3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690141a",
   "metadata": {},
   "source": [
    "### Построим пайпланы для SVM, RandomForest, Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcf2f4",
   "metadata": {},
   "source": [
    "Построим пайплайны с тремя предложенными к рассмотрению моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e77a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline(\n",
    "    [\n",
    "        ('stc', StandardScaler()),\n",
    "        ('SVC', SVC())\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline(\n",
    "    [\n",
    "        ('stc', StandardScaler()),\n",
    "        ('RF', RandomForestClassifier(random_state=100))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_cat = Pipeline(\n",
    "    [\n",
    "        ('CAT', CatBoostClassifier(verbose=False))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f777ee",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a053f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на тренировочной выборке составило 0.889\n",
      "Accuracy на тестовой выборке составило 0.89\n"
     ]
    }
   ],
   "source": [
    "svm_parameters_grid = {\n",
    "    'SVC__C': [1, 0.5, 3],\n",
    "    'SVC__kernel': ['linear', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "search_svm = GridSearchCV(\n",
    "    pipeline_svm,\n",
    "    svm_parameters_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "search_svm.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "pipeline_svm.set_params(**search_svm.best_params_)\n",
    "\n",
    "pipeline_svm.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "train_preds = pipeline_svm.predict(df_train.drop('Default', axis=1)) \n",
    "train_accuracy = np.mean(train_preds == df_train['Default'].values)\n",
    "\n",
    "test_preds = pipeline_svm.predict(df_test.drop('Default', axis=1)) \n",
    "test_accuracy = np.mean(test_preds == df_test['Default'].values)\n",
    "\n",
    "print(f\"Accuracy на тренировочной выборке составило {np.round(train_accuracy, decimals=3)}\")\n",
    "print(f\"Accuracy на тестовой выборке составило {np.round(test_accuracy, decimals=3)}\")\n",
    "\n",
    "with open('svm_model.pkl', 'wb') as f:\n",
    "     pickle.dump(pipeline_svm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fe9b3",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e9224cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на тренировочной выборке составило 0.923\n",
      "Accuracy на тестовой выборке составило 0.891\n"
     ]
    }
   ],
   "source": [
    "rf_parameters_grid = {\n",
    "    'RF__n_estimators': [100, 200, 300],\n",
    "    'RF__max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "search_rf = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    rf_parameters_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "search_rf.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "pipeline_rf.set_params(**search_rf.best_params_)\n",
    "\n",
    "pipeline_rf.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "train_preds = pipeline_rf.predict(df_train.drop('Default', axis=1)) \n",
    "train_accuracy = np.mean(train_preds == df_train['Default'].values)\n",
    "\n",
    "test_preds = pipeline_rf.predict(df_test.drop('Default', axis=1)) \n",
    "test_accuracy = np.mean(test_preds == df_test['Default'].values)\n",
    "\n",
    "print(f\"Accuracy на тренировочной выборке составило {np.round(train_accuracy, decimals=3)}\")\n",
    "print(f\"Accuracy на тестовой выборке составило {np.round(test_accuracy, decimals=3)}\")\n",
    "\n",
    "with open('rf_model.pkl', 'wb') as f:\n",
    "     pickle.dump(pipeline_rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3617f5e",
   "metadata": {},
   "source": [
    "Случайный лес справляется лучше на трейне и так же на тесте, но на тестовом датасете он все же показал лучший результат и был выбран."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02608f0",
   "metadata": {},
   "source": [
    "### Смотрим на значимость признаков для модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8e8b6",
   "metadata": {},
   "source": [
    "Посмотрим на значимость признаков для модели, потом используя эти данные пересоберем датасет и переобучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9280821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fdd-Dd                       0.115411\n",
       "Dd-fdd                       0.113710\n",
       "Ff                           0.086549\n",
       "hh                           0.065302\n",
       "First_default_date_time      0.061473\n",
       "First_deal_date_time         0.060896\n",
       "Secret_dwarf_info_1          0.046633\n",
       "Secret_dwarf_info_2          0.044993\n",
       "Secret_dwarf_info_3          0.039245\n",
       "Hashed_deal_detail_3         0.036279\n",
       "Region                       0.035892\n",
       "Successful_deals_count       0.033184\n",
       "Age_1.0                      0.031997\n",
       "Gender_Male                  0.026400\n",
       "Hashed_deal_detail_4         0.019676\n",
       "First_deal_date_month_7      0.014966\n",
       "Deal_date_weekday_2          0.014628\n",
       "Deal_date_month_11           0.011313\n",
       "Deal_date_weekday_4          0.009847\n",
       "Deal_date_year_2016          0.009602\n",
       "First_deal_date_year_2016    0.007678\n",
       "Deal_date_month_12           0.006888\n",
       "Deal_date_month_10           0.006667\n",
       "Deal_date_weekday_6          0.006331\n",
       "Deal_date_weekday_5          0.006065\n",
       "Deal_date_weekday_3          0.005902\n",
       "First_deal_date_month_2      0.005900\n",
       "First_deal_date_month_6      0.005765\n",
       "Deal_date_month_6            0.005539\n",
       "Deal_date_month_9            0.005508\n",
       "Deal_date_month_5            0.005424\n",
       "Deal_date_weekday_1          0.005368\n",
       "Deal_date_month_8            0.004954\n",
       "Deal_date_month_2            0.004934\n",
       "Deal_date_month_4            0.004779\n",
       "First_deal_date_month_4      0.004507\n",
       "First_deal_date_month_8      0.004304\n",
       "Deal_date_month_7            0.004177\n",
       "First_deal_date_month_10     0.004091\n",
       "First_deal_date_month_11     0.003739\n",
       "First_deal_date_month_3      0.003368\n",
       "Deal_date_month_3            0.002699\n",
       "First_deal_date_month_9      0.002637\n",
       "First_deal_date_month_12     0.001721\n",
       "Age_2.0                      0.001597\n",
       "First_deal_date_month_5      0.001462\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_scores = pd.Series(pipeline_rf.steps[1][1].feature_importances_, index=df_train.drop('Default', axis=1).columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7ae41",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0271d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на тренировочной выборке составило 0.917\n",
      "Accuracy на тестовой выборке составило 0.888\n"
     ]
    }
   ],
   "source": [
    "cat_parameters_grid = {\n",
    "    'CAT__n_estimators': [50, 100, 20],\n",
    "    'CAT__max_depth': [5, 10, 30]\n",
    "}\n",
    "\n",
    "search_cat = GridSearchCV(\n",
    "    pipeline_cat,\n",
    "    cat_parameters_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "search_cat.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "pipeline_cat.set_params(**search_cat.best_params_)\n",
    "\n",
    "pipeline_cat.fit(df_train.drop('Default', axis=1), df_train['Default'])\n",
    "\n",
    "train_preds = pipeline_cat.predict(df_train.drop('Default', axis=1)) \n",
    "train_accuracy = np.mean(train_preds == df_train['Default'].values)\n",
    "\n",
    "test_preds = pipeline_cat.predict(df_test.drop('Default', axis=1)) \n",
    "test_accuracy = np.mean(test_preds == df_test['Default'].values)\n",
    "\n",
    "print(f\"Accuracy на тренировочной выборке составило {np.round(train_accuracy, decimals=3)}\")\n",
    "print(f\"Accuracy на тестовой выборке составило {np.round(test_accuracy, decimals=3)}\")\n",
    "\n",
    "with open('cat_model.pkl', 'wb') as f:\n",
    "     pickle.dump(pipeline_cat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfad8c",
   "metadata": {},
   "source": [
    "### Сохранение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce37252",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline_rf.predict_proba(test_df.drop(['Default'], axis=1))[:, 1]\n",
    "test_df['Prediction'] = preds\n",
    "\n",
    "test_df['Prediction'].to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2199e14",
   "metadata": {},
   "source": [
    "### В какую сторону можно искать улучшения?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b000f2",
   "metadata": {},
   "source": [
    "Во-первых, необходимо лучше отобрать признаки, применив стратегии отбора: Forward selection, Backward selection, RFE.\n",
    "\n",
    "Во-вторых, можно продолжить эксперименты с моделями и посмотреть побольше в сторону ансамблей.\n",
    "\n",
    "Наконец, есть множество других способов классификации: нейросетевой подход, LDA, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
